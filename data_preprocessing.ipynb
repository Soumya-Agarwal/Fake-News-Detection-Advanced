{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/embibe/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import os \n",
    "import re\n",
    "from sklearn import preprocessing \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import f1_score, accuracy_score , recall_score , precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import nltk\n",
    "from nltk import tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/home/embibe/Personal/ML/NUS/LIAR-PLUS-master\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing a dictionary with word as key and its emotion as the value using the **NRC-emotion-lexicon-wordlevel-alphabetized-v0.92.txt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:4: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "filepath = \"NRC-emotion-lexicon-wordlevel-alphabetized-v0.92.txt\"\n",
    "emolex_df = pd.read_csv(filepath,  names=[\"word\", \"emotion\", \"association\"], skiprows=45, sep='\\t')\n",
    "emolex_df=emolex_df.loc[emolex_df['association']==1]\n",
    "emolex_dict=emolex_df[['word','emotion']].set_index('word').T.to_dict('list')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of forbidden words has been made from the forbidden.txt file provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "forbidden_words=[]\n",
    "with open(\"forbidden_words.txt\",\"r\") as f:\n",
    "    for line in f:\n",
    "        forbidden_words.append(line.replace('\\n',''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The given datasets have been loaded for preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers=['id','label_multiclass','statement','subject','speaker','speaker_job','state',\n",
    "        'party','barely_true_counts','false_counts','half_true_counts',\n",
    "        'mostly_true_counts','pants_on_fire_counts','venue','justification']\n",
    "\n",
    "train=pd.read_csv(\"train2.tsv\",names=headers,sep='\\t')\n",
    "val=pd.read_csv(\"val2.tsv\",names=headers,sep='\\t')\n",
    "test=pd.read_csv(\"test2.tsv\",names=headers,sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocess function converts to lowercase and removes any non alphabetical character from the category names. This is done because in the original dataframe same category is named in different ways. E.g. **U.S.President and us-president**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    for column in categorical_columns:\n",
    "        df[column]=df[column].str.replace('-', \" \")\n",
    "        df[column]=df[column].str.replace('.', \"\")\n",
    "        df[column]=df[column].str.lower()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean Function removes the forbidden word containing statements from justifications to remove any bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(x):\n",
    "    sentences=tokenize.sent_tokenize(x)\n",
    "    for index,sentence in enumerate(sentences):\n",
    "        if any(word in sentence for word in forbidden_words):\n",
    "            sentences.pop(index)\n",
    "    x=' '.join(sentences)\n",
    "    return x.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In calculate emotion function, the emolex dictionary is used to find the emotion of a statement to be identified as true, barely_true and so on. The words which didn't find an emotion in the dictionary are given a tag \"can't say\" emotion. For the entire statement, a list of emotions is made corresponding to each word in the statement. The emotion that repeats the most is taken as the emotion for that statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_emotion(news_statement):\n",
    "    emotion=[]\n",
    "    news_statement=re.sub('[^A-Za-z0-9\\s]+',\"\",news_statement).split()\n",
    "    for word in news_statement:\n",
    "        if word in emolex_dict.keys():\n",
    "            emotion.append(emolex_dict[word][0])\n",
    "        else:\n",
    "            continue\n",
    "    if(len(emotion))==0:\n",
    "        return (\"cant_say\")\n",
    "    else:\n",
    "        return max(emotion,key=emotion.count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The multiclass labels for a statement are grouped into two i.e. False and True to give each statement a binary label along with the multiclass labels.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_binary(df):\n",
    "    df_binary=pd.DataFrame()\n",
    "    df_binary['label']=df['label_multiclass']\n",
    "    df_binary.loc[(df_binary['label']=='false') | (df_binary['label']=='pants-fire') | (df_binary['label']=='barely-true')]='false'\n",
    "    df_binary.loc[(df_binary['label']=='half-true') | (df_binary['label']=='mostly-true') | (df_binary['label']=='true')]='true'\n",
    "    return df_binary['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The below two functions deal with the categorical columns. Top 98.5 percentile of the categories are taken and the rest of them are replaced with \"other\". Separate functions are made for train and val/test to ensure same categories in all the dataframes. Only the categories present in the train data are considered in val and test data. Any new category encountered has been replaced with \"other\".**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_with_other_1(df):\n",
    "    for x in categorical_columns:\n",
    "        value = df[x].value_counts()\n",
    "        df[x] = df[x].replace({x: 'other' for x in value[value < np.percentile(df[x].value_counts().values,98.5)].index})\n",
    "    return df\n",
    "\n",
    "def replace_with_other_2(df):\n",
    "    for x in categorical_columns:\n",
    "        df[x] = df[x].apply(lambda a: a if a in train_categorical[x].unique().tolist() else 'other')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['justification']=train['justification'].apply(lambda x: clean(str(x)))\n",
    "val['justification']=val['justification'].apply(lambda x: clean(str(x)))\n",
    "test['justification']=test['justification'].apply(lambda x: clean(str(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column values with empty justification have been replaced with \"unavailable\" to avoid NA condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "train.loc[train['justification']==\"\"]['justification']=\"unavailable\"\n",
    "val.loc[val['justification']==\"\"]['justification']=\"unavailable\"\n",
    "test.loc[test['justification']==\"\"]['justification']=\"unavailable\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([1425, 8400, 8493], dtype='int64')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[train['justification'] == ''].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(['id'],axis=1,inplace=True)\n",
    "val.drop(['id'],axis=1,inplace=True)\n",
    "test.drop(['id'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['emotion']=train['statement'].apply(lambda x: calculate_emotion(x))\n",
    "val['emotion']=val['statement'].apply(lambda x: calculate_emotion(x))\n",
    "test['emotion']=test['statement'].apply(lambda x: calculate_emotion(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['label_binary']=convert_to_binary(train)\n",
    "val['label_binary']=convert_to_binary(val)\n",
    "test['label_binary']=convert_to_binary(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10240, 16), (1284, 16), (1267, 16))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape,val.shape,test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The differnt columns in the data has been divided into numerical, categorical and text to deal with separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns = list(train.select_dtypes(include=['float64','int64']).columns)\n",
    "categorical_columns = list(train.select_dtypes(include=['object']).columns)\n",
    "text_columns=['statement','justification']\n",
    "categorical_columns = [column for column in categorical_columns if column not in text_columns]\n",
    "categorical_columns.remove(\"label_multiclass\")\n",
    "categorical_columns.remove(\"label_binary\")\n",
    "label_multiclass=['label_multiclass']\n",
    "label_binary=['label_binary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=preprocess(train)\n",
    "val=preprocess(val)\n",
    "test=preprocess(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "train_categorical = replace_with_other_1(train[categorical_columns])\n",
    "val_categorical = replace_with_other_2(val[categorical_columns])\n",
    "test_categorical = replace_with_other_2(test[categorical_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['false', 'half-true', 'mostly-true', 'true', 'barely-true',\n",
       "        'pants-fire'], dtype=object), array(['false', 'true'], dtype=object))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['label_multiclass'].unique(),train['label_binary'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['subject', 'speaker', 'speaker_job', 'state', 'party', 'venue', 'emotion']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empty categorical columns have been replaced with \"unknown\" to avoid NA condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/pandas/core/frame.py:4034: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  downcast=downcast, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "train_categorical.fillna(\"unknown\",inplace=True)\n",
    "val_categorical.fillna(\"unknown\",inplace=True)\n",
    "test_categorical.fillna(\"unknown\",inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the categorical columns have been one hot encoded to convert them to 0-1 format for right training. **Didn't do label encoding as all the categories are independent and there is no inter relation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_one_hot=pd.get_dummies(train_categorical, columns=categorical_columns, prefix=categorical_columns)\n",
    "val_one_hot=pd.get_dummies(val_categorical, columns=categorical_columns, prefix=categorical_columns)\n",
    "test_one_hot=pd.get_dummies(test_categorical, columns=categorical_columns, prefix=categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'party_unknown',\n",
       "  'speaker_bob mcdonnell',\n",
       "  'speaker_unknown',\n",
       "  'subject_medicare',\n",
       "  'subject_unknown',\n",
       "  'venue_a video ad',\n",
       "  'venue_an email',\n",
       "  'venue_press release'},\n",
       " {'party_unknown',\n",
       "  'speaker_rush limbaugh',\n",
       "  'speaker_unknown',\n",
       "  'subject_unknown',\n",
       "  'venue_a newspaper article',\n",
       "  'venue_a video ad',\n",
       "  'venue_an article',\n",
       "  'venue_an email blast'})"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(train_one_hot.columns)-set(test_one_hot.columns),set(train_one_hot.columns)-set(val_one_hot.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get missing columns in the validation and test dataset w.r.t the train dataset. Add missing columns in test and val set with default value equal to 0. Ensure the order of column in the test and val set is in the same order as train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_cols = set( train_one_hot.columns ) - set( test_one_hot.columns )\n",
    "for c in missing_cols:\n",
    "    test_one_hot[c] = 0\n",
    "\n",
    "test_one_hot = test_one_hot[train_one_hot.columns]\n",
    "\n",
    "missing_cols = set( train_one_hot.columns ) - set( val_one_hot.columns )\n",
    "\n",
    "for c in missing_cols:\n",
    "    val_one_hot[c] = 0\n",
    "\n",
    "val_one_hot = val_one_hot[train_one_hot.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multiclass labels have been label encoded as {0,1,2,3,4,5} to prepare the preprocessed dataset.\n",
    "The binary labels have been label encoded as {0,1}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "le=preprocessing.LabelEncoder()\n",
    "train['label_multiclass']=le.fit_transform(train['label_multiclass'])\n",
    "val['label_multiclass']=le.transform(val['label_multiclass'])\n",
    "test['label_multiclass']=le.transform(test['label_multiclass'])\n",
    "train['label_binary']=le.fit_transform(train['label_binary'])\n",
    "val['label_binary']=le.transform(val['label_binary'])\n",
    "test['label_binary']=le.transform(test['label_binary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the prepared features are concatenated together to make the final train, validation and test dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final = pd.concat([train[text_columns],train_one_hot,train[numerical_columns],train[label_multiclass],train[label_binary]],axis=1)\n",
    "val_final = pd.concat([val[text_columns],val_one_hot,val[numerical_columns],val[label_multiclass],val[label_binary]],axis=1)\n",
    "test_final = pd.concat([test[text_columns],test_one_hot,test[numerical_columns],test[label_multiclass],test[label_binary]],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10240, 207), (1284, 207), (1267, 207))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_final.shape,val_final.shape,test_final.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is ensured that all the Na values if any left are replaced with 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final.fillna(0,inplace=True)\n",
    "val_final.fillna(0,inplace=True)\n",
    "test_final.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessed csv's have been made to for direct access while modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final.to_csv(\"train_preprocessed.csv\",index=False)\n",
    "val_final.to_csv(\"val_preprocessed.csv\",index=False)\n",
    "test_final.to_csv(\"test_preprocessed.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze > requirements_preprocessing.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
