{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from __future__ import absolute_import\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, TimeDistributed, Dense, Lambda, concatenate, Dropout, BatchNormalization\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from sklearn.metrics import classification_report\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn import preprocessing\n",
    "import functools\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score, accuracy_score , recall_score , precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/home/embibe/Personal/ML/NUS/LIAR-PLUS-master\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following is the function to print the classification and confusion report for the prepared model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_and_confusion_report_binary(actual_label,predicted_label,threshold):\n",
    "    predicted_label = np.where(predicted_label>threshold,1,0)\n",
    "    report=classification_report(actual_label,predicted_label)\n",
    "    cm = confusion_matrix(actual_label,predicted_label)\n",
    "    print(cm)\n",
    "    print(report)\n",
    "    return(predicted_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where it comes in handy since we need to “merge” our two LSTMs output using the MaLSTM(Manhattan LSTM) similarity function. It tells the difference between the two feature vectors of the statement and justification generated by the siamese network.The function is made such that value remains between 0 to 1 as function is e^(-x) where x is always positive as it is the manhattan distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponent_neg_manhattan_distance(left, right):\n",
    "    return K.exp(-K.sum(K.abs(left-right), axis=1, keepdims=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading all the preprocessed files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv(\"train_preprocessed.csv\")\n",
    "val=pd.read_csv(\"val_preprocessed.csv\")\n",
    "test=pd.read_csv(\"test_preprocessed.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting the features and labels from the loaded data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train.drop(['label_multiclass','label_binary'],axis=1)\n",
    "x_val = val.drop(['label_multiclass','label_binary'],axis=1)\n",
    "x_test = test.drop(['label_multiclass','label_binary'],axis=1)\n",
    "y_train_multiclass = train['label_multiclass']\n",
    "y_test_multiclass = test['label_multiclass']\n",
    "y_val_multiclass = val['label_multiclass']\n",
    "y_train_binary=train['label_binary']\n",
    "y_val_binary=val['label_binary']\n",
    "y_test_binary=test['label_binary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10240, 205), (1284, 205), (1267, 205))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape,x_val.shape,x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the approximate length of the justification and statement column for padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 11)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train['justification'][0].split()),len(x_train['statement'][0].split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(40+11)*10240=522240 --->> approx words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sentence length is approx from 0 to 20 in statement and 0 to 60 in justification and hence the maximum sequence length is set to 20 and 60 respectively. The embedding used is of dimension 50 from glove for each word. All the important variables are initialized in the following cell.\n",
    "**Num of epochs has been set to 20 due to resource constraints. Increasing epochs shall improve learning further.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_words = 25900 \n",
    "EMBEDDING_DIM = 50\n",
    "MAX_SEQUENCE_LENGTH_1=60\n",
    "MAX_SEQUENCE_LENGTH_2=60\n",
    "DROPOUT = 0.1\n",
    "num_epoch=20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer is initialized and is fitted on the train set and then the train and validation data is converted to sequences based on the learnt tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer() \n",
    "tokenizer.fit_on_texts(np.concatenate((x_train.statement.astype(str), x_train.justification.astype(str)), axis=0))\n",
    "\n",
    "sequences_train_q1 = tokenizer.texts_to_sequences(x_train.statement.astype(str))\n",
    "sequences_train_q2 = tokenizer.texts_to_sequences(x_train.justification.astype(str))\n",
    "\n",
    "sequences_val_q1 = tokenizer.texts_to_sequences(x_val.statement.astype(str))\n",
    "sequences_val_q2 = tokenizer.texts_to_sequences(x_val.justification.astype(str))\n",
    "\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the sequences are padded to a maximum length of 20 and 60 as mentioned above in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_train_q1_padded = pad_sequences(sequences_train_q1, maxlen=MAX_SEQUENCE_LENGTH_1)\n",
    "sequences_train_q2_padded = pad_sequences(sequences_train_q2, maxlen=MAX_SEQUENCE_LENGTH_2)\n",
    "\n",
    "sequences_val_q1_padded = pad_sequences(sequences_val_q1, maxlen=MAX_SEQUENCE_LENGTH_1)\n",
    "sequences_val_q2_padded = pad_sequences(sequences_val_q2, maxlen=MAX_SEQUENCE_LENGTH_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WORD EMBEDDING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embedding matrix is prepared using the glove file of 50 dimension for each word. A dictionary with keys as words and values as embeddings is prepared from the glove file and the words that we have in our vocabulary from tokenizer are given those embeddings and saved in the embedding matrix according to index. We have matrix size of NUM_WORDS*EMBEDDING_DIM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding_matrix = np.zeros((nb_words+1, EMBEDDING_DIM))\n",
    "\n",
    "embeddings_index = {}\n",
    "\n",
    "f = open('glove.6B.50d.txt')\n",
    "\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]       \n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "    \n",
    "f.close()\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i >= nb_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        word_embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features=x_train.drop(['statement','justification'],axis=1)\n",
    "val_features=x_val.drop(['statement','justification'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10240, 203), (1284, 203))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape,val_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the labels are multiclass they are converted to one hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_onehot =pd.get_dummies(y_train_multiclass, columns=['label_multiclass'], prefix=['label_multiclass'])\n",
    "val_y_onehot = pd.get_dummies(y_val_multiclass, columns=['label_multiclass'], prefix=['label_multiclass'])\n",
    "test_y_onehot = pd.get_dummies(y_test_multiclass, columns=['label_multiclass'], prefix=['label_multiclass'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network's input dimensions are been given including the word embeddings and the other features calculated in data_preprocessing step. The weights have been initialized with the word embedding matrix obtained from glove. Bidirectional LSTMs have been used to learn the word embeddings.'sum' Merge_mode is used to combine outputs of the forward and backward RNNs. \n",
    "\n",
    "Since this is a siamese network, both sides share the same LSTM initialised with the same weights and having the same parameters giving different feature vectors for the two input questions to be compared. Lambda function calculates the distance as defined by the exponent_neg_manhattan_distance function and that distance is concatenated with our other features. Further Dense layers and Dropouts are added to give final output layer with 1 and 6 nodes respectively i.e is_true_2 and is_true_6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BINARY AND MULTICLASS CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0809 20:36:55.695868 140287669671744 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0809 20:36:55.744004 140287669671744 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0809 20:36:55.753084 140287669671744 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0809 20:36:55.785442 140287669671744 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0809 20:36:55.787733 140287669671744 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0809 20:36:58.849005 140287669671744 deprecation.py:506] From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0809 20:37:00.513576 140287669671744 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0809 20:37:00.766571 140287669671744 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_impl.py:180: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "statement = Input(shape=(MAX_SEQUENCE_LENGTH_1,))\n",
    "justification = Input(shape=(MAX_SEQUENCE_LENGTH_2,))\n",
    "\n",
    "q1 = Embedding(nb_words + 1, \n",
    "                 EMBEDDING_DIM, \n",
    "                 weights=[word_embedding_matrix], \n",
    "                 input_length=MAX_SEQUENCE_LENGTH_1, \n",
    "                 trainable=True)(statement)\n",
    "\n",
    "q2 = Embedding(nb_words + 1, \n",
    "                 EMBEDDING_DIM, \n",
    "                 weights=[word_embedding_matrix], \n",
    "                 input_length=MAX_SEQUENCE_LENGTH_2, \n",
    "                 trainable=True)(justification)\n",
    "\n",
    "shared_lstm = Bidirectional(LSTM(30), merge_mode=\"sum\")\n",
    "\n",
    "q1 = shared_lstm(q1)\n",
    "q2 = shared_lstm(q2)\n",
    "\n",
    "malstm_distance = Lambda(function=lambda x: exponent_neg_manhattan_distance(x[0], x[1]),output_shape=lambda x: (x[0][0],1))([q1,q2])\n",
    "\n",
    "features  = Input(shape=(train_features.shape[1],))\n",
    "\n",
    "merged = concatenate([q1,q2,malstm_distance,features])\n",
    "merged = Dense(200, activation='relu')(merged)\n",
    "merged = Dropout(DROPOUT)(merged)\n",
    "\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dense(200, activation='relu')(merged)\n",
    "merged = Dropout(DROPOUT)(merged)\n",
    "\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dense(200, activation='relu')(merged)\n",
    "merged = Dropout(DROPOUT)(merged)\n",
    "\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dense(200, activation='relu')(merged)\n",
    "merged = Dropout(DROPOUT)(merged)\n",
    "\n",
    "merged = BatchNormalization()(merged)\n",
    "\n",
    "is_true_6 = Dense(6, activation='softmax')(merged)\n",
    "is_true_2 = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "lstm_model_multiclass = Model(inputs=[statement,justification,features], outputs=[is_true_6])\n",
    "lstm_model_multiclass.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "lstm_model_binary = Model(inputs=[statement,justification,features], outputs=[is_true_2])\n",
    "lstm_model_binary.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 60)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 60)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 60, 50)       1295050     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 60, 50)       1295050     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 30)           19440       embedding_1[0][0]                \n",
      "                                                                 embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 1)            0           bidirectional_1[0][0]            \n",
      "                                                                 bidirectional_1[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 203)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 264)          0           bidirectional_1[0][0]            \n",
      "                                                                 bidirectional_1[1][0]            \n",
      "                                                                 lambda_1[0][0]                   \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 200)          53000       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 200)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 200)          800         dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 200)          40200       batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 200)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 200)          800         dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 200)          40200       batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 200)          0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 200)          800         dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 200)          40200       batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 200)          0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 200)          800         dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 6)            1206        batch_normalization_4[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 2,787,546\n",
      "Trainable params: 2,785,946\n",
      "Non-trainable params: 1,600\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm_model_multiclass.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 60)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 60)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 60, 50)       1295050     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 60, 50)       1295050     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 30)           19440       embedding_1[0][0]                \n",
      "                                                                 embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 1)            0           bidirectional_1[0][0]            \n",
      "                                                                 bidirectional_1[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 203)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 264)          0           bidirectional_1[0][0]            \n",
      "                                                                 bidirectional_1[1][0]            \n",
      "                                                                 lambda_1[0][0]                   \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 200)          53000       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 200)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 200)          800         dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 200)          40200       batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 200)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 200)          800         dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 200)          40200       batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 200)          0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 200)          800         dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 200)          40200       batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 200)          0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 200)          800         dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1)            201         batch_normalization_4[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 2,786,541\n",
      "Trainable params: 2,784,941\n",
      "Non-trainable params: 1,600\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm_model_binary.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model is fitted with input values as padded train and test sequences and output is given as is_true_6 and available validation set is provided while training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10240 samples, validate on 1284 samples\n",
      "Epoch 1/20\n",
      "10240/10240 [==============================] - 168s 16ms/step - loss: 1.7253 - acc: 0.3421 - val_loss: 1.4844 - val_acc: 0.4237\n",
      "Epoch 2/20\n",
      "10240/10240 [==============================] - 136s 13ms/step - loss: 1.4554 - acc: 0.4138 - val_loss: 1.3456 - val_acc: 0.4338\n",
      "Epoch 3/20\n",
      "10240/10240 [==============================] - 130s 13ms/step - loss: 1.3738 - acc: 0.4405 - val_loss: 1.3387 - val_acc: 0.4587\n",
      "Epoch 4/20\n",
      "10240/10240 [==============================] - 119s 12ms/step - loss: 1.3340 - acc: 0.4427 - val_loss: 1.3135 - val_acc: 0.4572\n",
      "Epoch 5/20\n",
      "10240/10240 [==============================] - 130s 13ms/step - loss: 1.2923 - acc: 0.4616 - val_loss: 1.3172 - val_acc: 0.4299\n",
      "Epoch 6/20\n",
      "10240/10240 [==============================] - 129s 13ms/step - loss: 1.2723 - acc: 0.4697 - val_loss: 1.3082 - val_acc: 0.4408\n",
      "Epoch 7/20\n",
      "10240/10240 [==============================] - 131s 13ms/step - loss: 1.2354 - acc: 0.4783 - val_loss: 1.3196 - val_acc: 0.4751\n",
      "Epoch 8/20\n",
      "10240/10240 [==============================] - 99s 10ms/step - loss: 1.2005 - acc: 0.4971 - val_loss: 1.3608 - val_acc: 0.4198\n",
      "Epoch 9/20\n",
      "10240/10240 [==============================] - 67s 7ms/step - loss: 1.1444 - acc: 0.5234 - val_loss: 1.3863 - val_acc: 0.4502\n",
      "Epoch 10/20\n",
      "10240/10240 [==============================] - 73s 7ms/step - loss: 1.0885 - acc: 0.5476 - val_loss: 1.4575 - val_acc: 0.4245\n",
      "Epoch 11/20\n",
      "10240/10240 [==============================] - 64s 6ms/step - loss: 0.9939 - acc: 0.5823 - val_loss: 1.5884 - val_acc: 0.4081\n",
      "Epoch 12/20\n",
      "10240/10240 [==============================] - 74s 7ms/step - loss: 0.8977 - acc: 0.6302 - val_loss: 1.6839 - val_acc: 0.4385\n",
      "Epoch 13/20\n",
      "10240/10240 [==============================] - 68s 7ms/step - loss: 0.7923 - acc: 0.6733 - val_loss: 1.8814 - val_acc: 0.4245\n",
      "Epoch 14/20\n",
      "10240/10240 [==============================] - 74s 7ms/step - loss: 0.6976 - acc: 0.7104 - val_loss: 1.9267 - val_acc: 0.4330\n",
      "Epoch 15/20\n",
      "10240/10240 [==============================] - 61s 6ms/step - loss: 0.6177 - acc: 0.7461 - val_loss: 2.1291 - val_acc: 0.4315\n",
      "Epoch 16/20\n",
      "10240/10240 [==============================] - 61s 6ms/step - loss: 0.5194 - acc: 0.7848 - val_loss: 2.3169 - val_acc: 0.4159\n",
      "Epoch 17/20\n",
      "10240/10240 [==============================] - 60s 6ms/step - loss: 0.4618 - acc: 0.8054 - val_loss: 2.4517 - val_acc: 0.3941\n",
      "Epoch 18/20\n",
      "10240/10240 [==============================] - 61s 6ms/step - loss: 0.4132 - acc: 0.8278 - val_loss: 2.6727 - val_acc: 0.3816\n",
      "Epoch 19/20\n",
      "10240/10240 [==============================] - 59s 6ms/step - loss: 0.3758 - acc: 0.8472 - val_loss: 2.7299 - val_acc: 0.4050\n",
      "Epoch 20/20\n",
      "10240/10240 [==============================] - 61s 6ms/step - loss: 0.3245 - acc: 0.8673 - val_loss: 2.8913 - val_acc: 0.4221\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f96f1bde110>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model_multiclass.fit([sequences_train_q1_padded, sequences_train_q2_padded,train_features], train_y_onehot, batch_size=64, nb_epoch=num_epoch,validation_data=([sequences_val_q1_padded, sequences_val_q2_padded,val_features],val_y_onehot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer is applied to test data and padding is done afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_test_data_q1 = tokenizer.texts_to_sequences(x_test.statement.astype(str))\n",
    "sequences_test_data_q2 = tokenizer.texts_to_sequences(x_test.justification.astype(str))\n",
    "sequences_test_data_q1_padded = pad_sequences(sequences_test_data_q1, maxlen=MAX_SEQUENCE_LENGTH_1)\n",
    "sequences_test_data_q2_padded = pad_sequences(sequences_test_data_q2, maxlen=MAX_SEQUENCE_LENGTH_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As statement and justification have been converted to embeddings the as it is text columns have been removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features=x_test.drop(['statement','justification'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_multiclass = lstm_model_multiclass.predict([sequences_test_data_q1_padded, sequences_test_data_q2_padded,test_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.17154920e-01, 5.72092421e-02, 2.59098131e-02, 6.92518777e-04,\n",
       "        1.98353916e-01, 6.79565128e-04],\n",
       "       [9.99786794e-01, 2.45780557e-05, 5.46478276e-08, 1.21944453e-07,\n",
       "        1.11018403e-06, 1.87421567e-04],\n",
       "       [4.42616552e-01, 1.71677917e-01, 8.81721731e-03, 1.57764554e-03,\n",
       "        3.74687433e-01, 6.23150496e-04],\n",
       "       ...,\n",
       "       [9.99961019e-01, 1.04740445e-06, 4.32311751e-08, 2.72947864e-09,\n",
       "        1.61755764e-07, 3.78213408e-05],\n",
       "       [8.36522598e-03, 8.75938237e-01, 5.95911220e-02, 3.92761634e-04,\n",
       "        5.53054065e-02, 4.07304004e-04],\n",
       "       [3.16324830e-01, 6.80857480e-01, 1.19067903e-03, 1.35656708e-04,\n",
       "        1.03779945e-04, 1.38755504e-03]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_multiclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **The class having maximum score in prediction is taken to be the predicted class.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_multiclass = y_pred_multiclass.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('accuracy', 0.40015785319652725)\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy\",accuracy_score(indices_multiclass,y_test_multiclass))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 84  32  70  10  14   2]\n",
      " [ 36 104  73  16  12   8]\n",
      " [ 34  41 143  30  11   6]\n",
      " [ 23  30  85  87   4  12]\n",
      " [ 10   9  17   3  50   3]\n",
      " [ 25  33  70  33   8  39]]\n"
     ]
    }
   ],
   "source": [
    "conf_mat = confusion_matrix(y_test_multiclass, indices_multiclass)\n",
    "print(conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.40      0.40       212\n",
      "           1       0.42      0.42      0.42       249\n",
      "           2       0.31      0.54      0.40       265\n",
      "           3       0.49      0.36      0.41       241\n",
      "           4       0.51      0.54      0.52        92\n",
      "           5       0.56      0.19      0.28       208\n",
      "\n",
      "   micro avg       0.40      0.40      0.40      1267\n",
      "   macro avg       0.45      0.41      0.40      1267\n",
      "weighted avg       0.43      0.40      0.39      1267\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report_multiclass=classification_report(y_test_multiclass, indices_multiclass)\n",
    "print(report_multiclass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model is fitted with input values as padded train and test sequences and output is given as is_true_2 and available validation set is provided while training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10240 samples, validate on 1284 samples\n",
      "Epoch 1/20\n",
      "10240/10240 [==============================] - 84s 8ms/step - loss: 0.1052 - acc: 0.9604 - val_loss: 1.2693 - val_acc: 0.6955\n",
      "Epoch 2/20\n",
      "10240/10240 [==============================] - 68s 7ms/step - loss: 0.0575 - acc: 0.9784 - val_loss: 1.3632 - val_acc: 0.7002\n",
      "Epoch 3/20\n",
      "10240/10240 [==============================] - 64s 6ms/step - loss: 0.0480 - acc: 0.9833 - val_loss: 1.5209 - val_acc: 0.6869\n",
      "Epoch 4/20\n",
      "10240/10240 [==============================] - 61s 6ms/step - loss: 0.0375 - acc: 0.9860 - val_loss: 1.5426 - val_acc: 0.6838\n",
      "Epoch 5/20\n",
      "10240/10240 [==============================] - 67s 7ms/step - loss: 0.0349 - acc: 0.9893 - val_loss: 1.6271 - val_acc: 0.6955\n",
      "Epoch 6/20\n",
      "10240/10240 [==============================] - 66s 6ms/step - loss: 0.0273 - acc: 0.9901 - val_loss: 1.6941 - val_acc: 0.6939\n",
      "Epoch 7/20\n",
      "10240/10240 [==============================] - 67s 7ms/step - loss: 0.0283 - acc: 0.9902 - val_loss: 1.5107 - val_acc: 0.7056\n",
      "Epoch 8/20\n",
      "10240/10240 [==============================] - 67s 7ms/step - loss: 0.0246 - acc: 0.9917 - val_loss: 1.6693 - val_acc: 0.7025\n",
      "Epoch 9/20\n",
      "10240/10240 [==============================] - 74s 7ms/step - loss: 0.0251 - acc: 0.9906 - val_loss: 1.7252 - val_acc: 0.6939\n",
      "Epoch 10/20\n",
      "10240/10240 [==============================] - 70s 7ms/step - loss: 0.0206 - acc: 0.9936 - val_loss: 1.6090 - val_acc: 0.7103\n",
      "Epoch 11/20\n",
      "10240/10240 [==============================] - 72s 7ms/step - loss: 0.0140 - acc: 0.9955 - val_loss: 1.9482 - val_acc: 0.6924\n",
      "Epoch 12/20\n",
      "10240/10240 [==============================] - 59s 6ms/step - loss: 0.0147 - acc: 0.9953 - val_loss: 1.7591 - val_acc: 0.6963\n",
      "Epoch 13/20\n",
      "10240/10240 [==============================] - 59s 6ms/step - loss: 0.0154 - acc: 0.9949 - val_loss: 1.6542 - val_acc: 0.7142\n",
      "Epoch 14/20\n",
      "10240/10240 [==============================] - 65s 6ms/step - loss: 0.0150 - acc: 0.9952 - val_loss: 1.7680 - val_acc: 0.7002\n",
      "Epoch 15/20\n",
      "10240/10240 [==============================] - 63s 6ms/step - loss: 0.0124 - acc: 0.9961 - val_loss: 1.8274 - val_acc: 0.6838\n",
      "Epoch 16/20\n",
      "10240/10240 [==============================] - 63s 6ms/step - loss: 0.0136 - acc: 0.9964 - val_loss: 2.0663 - val_acc: 0.7025\n",
      "Epoch 17/20\n",
      "10240/10240 [==============================] - 64s 6ms/step - loss: 0.0151 - acc: 0.9949 - val_loss: 1.7161 - val_acc: 0.7002\n",
      "Epoch 18/20\n",
      "10240/10240 [==============================] - 64s 6ms/step - loss: 0.0154 - acc: 0.9940 - val_loss: 1.8813 - val_acc: 0.7040\n",
      "Epoch 19/20\n",
      "10240/10240 [==============================] - 64s 6ms/step - loss: 0.0153 - acc: 0.9945 - val_loss: 1.9872 - val_acc: 0.6970\n",
      "Epoch 20/20\n",
      "10240/10240 [==============================] - 66s 6ms/step - loss: 0.0117 - acc: 0.9958 - val_loss: 1.9021 - val_acc: 0.7017\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f969e7d4d10>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model_binary.fit([sequences_train_q1_padded, sequences_train_q2_padded,train_features], y_train_binary, batch_size=64, nb_epoch=num_epoch,validation_data=([sequences_val_q1_padded, sequences_val_q2_padded,val_features],y_val_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model_multiclass.save('multiclass_model_siamese.h5')\n",
    "lstm_model_binary.save('binary_model_siamese.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_binary = lstm_model_binary.predict([sequences_test_data_q1_padded, sequences_test_data_q2_padded,test_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Threshold is taken as 0.5. Those with value less than 0.5 are considered of label 0, 1 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[378 175]\n",
      " [216 498]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.68      0.66       553\n",
      "           1       0.74      0.70      0.72       714\n",
      "\n",
      "   micro avg       0.69      0.69      0.69      1267\n",
      "   macro avg       0.69      0.69      0.69      1267\n",
      "weighted avg       0.69      0.69      0.69      1267\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted_label=classification_and_confusion_report_binary(y_test_binary,y_pred_binary,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('accuracy', 0.691397000789266)\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy\",accuracy_score(predicted_label,y_test_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.68      0.66       553\n",
      "           1       0.74      0.70      0.72       714\n",
      "\n",
      "   micro avg       0.69      0.69      0.69      1267\n",
      "   macro avg       0.69      0.69      0.69      1267\n",
      "weighted avg       0.69      0.69      0.69      1267\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report_binary=classification_report(y_test_binary, predicted_label)\n",
    "print(report_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze > requirements_Siamese.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
