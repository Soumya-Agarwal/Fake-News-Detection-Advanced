{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import os\n",
    "from sklearn import preprocessing \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from sklearn.metrics import f1_score, accuracy_score , recall_score , precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/home/embibe/Personal/ML/NUS/LIAR-PLUS-master\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading all the preprocessed files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv(\"train_preprocessed.csv\")\n",
    "val=pd.read_csv(\"val_preprocessed.csv\")\n",
    "test=pd.read_csv(\"test_preprocessed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10240, 207), (1284, 207), (1267, 207))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape,val.shape,test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to initialize tf-idf and fit it on the train set and then transform the test and validation data to sequences based on the learnt tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(df,flag):\n",
    "    if (flag=='train'):\n",
    "        xyz = np.concatenate([tfidf_statement.fit_transform(df['statement'].values).toarray().tolist(),\n",
    "                         tfidf_justification.fit_transform(df['justification'].values.astype('U')).toarray().tolist()]\n",
    "                         ,axis=1)\n",
    "    else:\n",
    "        xyz = np.concatenate([tfidf_statement.transform(df['statement'].values).toarray().tolist(),\n",
    "                         tfidf_justification.transform(df['justification'].values.astype('U')).toarray().tolist()]\n",
    "                         ,axis=1)\n",
    "        \n",
    "    return xyz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting the features and labels from the loaded data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train.drop(['label_multiclass','label_binary'],axis=1)\n",
    "x_val = val.drop(['label_multiclass','label_binary'],axis=1)\n",
    "x_test = test.drop(['label_multiclass','label_binary'],axis=1)\n",
    "y_train_multiclass = train['label_multiclass']\n",
    "y_test_multiclass = test['label_multiclass']\n",
    "y_val_multiclass = val['label_multiclass']\n",
    "y_train_binary=train['label_binary']\n",
    "y_val_binary=val['label_binary']\n",
    "y_test_binary=test['label_binary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10240, 205), (1284, 205), (1267, 205))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape,x_val.shape,x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 2, 3, 5, 0, 4]), array([0, 1]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_multiclass.unique(),y_train_binary.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_statement = TfidfVectorizer(lowercase=True,ngram_range=(1,3),max_df=0.9, min_df=0.1)\n",
    "tfidf_justification = TfidfVectorizer(lowercase=True,ngram_range=(1,3),max_df=0.9, min_df=0.1)\n",
    "\n",
    "train_tfidf = pd.DataFrame(tf_idf(x_train,'train'))\n",
    "val_tfidf= pd.DataFrame(tf_idf(x_val,'val'))\n",
    "test_tfidf=  pd.DataFrame(tf_idf(x_test,'test'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original metadata is concatenated with the newly generated tfidf features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = pd.concat([train_tfidf,x_train],axis=1)\n",
    "val_features = pd.concat([val_tfidf,x_val],axis=1)\n",
    "test_features = pd.concat([test_tfidf,x_test],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As statement and justification have been converted to vectors the as it is text columns have been removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features.drop(['statement','justification'],axis=1,inplace=True)\n",
    "val_features.drop(['statement','justification'],axis=1,inplace=True)\n",
    "test_features.drop(['statement','justification'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10240, 290), (1284, 290), (1267, 290))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape,val_features.shape,test_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_multiclass = RandomForestClassifier(n_estimators=200, oob_score='TRUE', n_jobs=-1, random_state=50, max_features=\"auto\",min_samples_leaf=1)\n",
    "model_multiclass.fit(train_features, y_train_multiclass)\n",
    "y_pred_multiclass = model_multiclass.predict(test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Achieved 40% accuracy just by learning on train data. If validation is also passed, parameters will be tuned and accuracy shall improve further.Grid search has been bypassed due to time and resource constraints and had that being done, accuracy will improve further with the best parameters chosen.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('accuracy of Random Forest:', 0.40331491712707185)\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy of Random Forest:\",accuracy_score(y_pred_multiclass,y_test_multiclass))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 64  39  56  42   6   5]\n",
      " [ 25 127  39  39  13   6]\n",
      " [ 26  50 116  62   2   9]\n",
      " [ 17  27  71 117   3   6]\n",
      " [  3  20  16   6  47   0]\n",
      " [ 15  29  58  62   4  40]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "conf_mat = confusion_matrix(y_test_multiclass, y_pred_multiclass)\n",
    "print(conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.30      0.35       212\n",
      "           1       0.43      0.51      0.47       249\n",
      "           2       0.33      0.44      0.37       265\n",
      "           3       0.36      0.49      0.41       241\n",
      "           4       0.63      0.51      0.56        92\n",
      "           5       0.61      0.19      0.29       208\n",
      "\n",
      "   micro avg       0.40      0.40      0.40      1267\n",
      "   macro avg       0.46      0.41      0.41      1267\n",
      "weighted avg       0.44      0.40      0.40      1267\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report_multiclass=classification_report(y_test_multiclass, y_pred_multiclass)\n",
    "print(report_multiclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_binary = RandomForestClassifier(n_estimators=400, oob_score='TRUE', n_jobs=-1, random_state=50, max_features=\"auto\",min_samples_leaf=1)\n",
    "model_binary.fit(train_features, y_train_binary)\n",
    "y_pred_binary = model_binary.predict(test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Achieved 72% accuracy just by learning on train data. If validation is also passed, parameters will be tuned and accuracy shall improve further.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('accuracy of Random Forest:', 0.7277032359905288)\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy of Random Forest:\",accuracy_score(y_pred_binary,y_test_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[315 238]\n",
      " [107 607]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "conf_mat = confusion_matrix(y_test_binary, y_pred_binary)\n",
    "print(conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.57      0.65       553\n",
      "           1       0.72      0.85      0.78       714\n",
      "\n",
      "   micro avg       0.73      0.73      0.73      1267\n",
      "   macro avg       0.73      0.71      0.71      1267\n",
      "weighted avg       0.73      0.73      0.72      1267\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report_binary=classification_report(y_test_binary, y_pred_binary)\n",
    "print(report_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before inserting into SVM all the features are scaled so as to be consistent with the weightage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/base.py:464: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:4: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  after removing the cwd from sys.path.\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:5: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(train_features)\n",
    "x_val_scaled = scaler.transform(val_features)\n",
    "x_test_scaled = scaler.transform(test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For hyperparameter tuning. **The model is not trained due to resource constraints.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_grid = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],\n",
    "                     'C': [1, 10, 100, 1000]},\n",
    "                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze > requirements_tfidf.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An attempt to add validation data in SVM for improving results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import PredefinedSplit\n",
    "# X = pd.concat(x_train_scaled,x_val_scaled)\n",
    "# y = pd.concat(y_train_multiclass,y_val_multiclass)\n",
    "# test_fold = [0, 1, -1, 1]\n",
    "# ps = PredefinedSplit(test_fold)\n",
    "# ps.get_n_splits()\n",
    "# print(ps)       \n",
    "\n",
    "# PredefinedSplit(test_fold=array([ 0,  1, -1,  1]))\n",
    "# for train_index, test_index in ps.split():\n",
    "#     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "#     X_train, X_test = X[train_index], X[test_index]\n",
    "#     y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model_multiclass = GridSearchCV(SVC(), params_grid, cv=5)\n",
    "svm_model_multiclass.fit(x_train_scaled, y_train_multiclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best score for training data:', svm_model_multiclass.best_score_,\"\\n\") \n",
    "\n",
    "# View the best parameters for the model found using grid search\n",
    "print('Best C:',svm_model_multiclass.best_estimator_.C,\"\\n\") \n",
    "print('Best Kernel:',svm_model_multiclass.best_estimator_.kernel,\"\\n\")\n",
    "print('Best Gamma:',svm_model_multiclass.best_estimator_.gamma,\"\\n\")\n",
    "\n",
    "final_model_multiclass = svm_model_multiclass.best_estimator_\n",
    "y_pred_multiclass = final_model_multiclass.predict(x_test_scaled)\n",
    "y_pred_multiclass = list(encoder.inverse_transform(y_pred_multiclass))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test,y_pred_multiclass))\n",
    "print(\"\\n\")\n",
    "print(classification_report(y_test,y_pred_multiclass))\n",
    "\n",
    "print(\"Training set score for SVM: %f\" % final_model_multiclass.score(x_train_scaled , y_train))\n",
    "print(\"Testing  set score for SVM: %f\" % final_model_multiclass.score(x_test_scaled  , y_test ))\n",
    "\n",
    "svm_model_multiclass.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model_binary = GridSearchCV(SVC(), params_grid, cv=5)\n",
    "svm_model_binary.fit(x_train_scaled, train_y_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best score for training data:', svm_model_binary.best_score_,\"\\n\") \n",
    "\n",
    "# View the best parameters for the model found using grid search\n",
    "print('Best C:',svm_model_binary.best_estimator_.C,\"\\n\") \n",
    "print('Best Kernel:',svm_model_binary.best_estimator_.kernel,\"\\n\")\n",
    "print('Best Gamma:',svm_model_binary.best_estimator_.gamma,\"\\n\")\n",
    "\n",
    "final_model_binary = svm_model_binary.best_estimator_\n",
    "y_pred_binary = final_model_binary.predict(x_test_scaled)\n",
    "y_pred_binary = list(encoder.inverse_transform(y_pred_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(test_y_binary,y_pred_binary))\n",
    "print(\"\\n\")\n",
    "print(classification_report(test_y_binary,y_pred_binary))\n",
    "\n",
    "print(\"Training set score for SVM: %f\" % final_model_binary.score(x_train_scaled ,train_y_binary))\n",
    "print(\"Testing  set score for SVM: %f\" % final_model_binary.score(x_test_scaled  , test_y_binary ))\n",
    "\n",
    "svm_model_binary.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
